<!DOCTYPE html>

<html lang="en-us"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Nanum+Myeongjo&family=Noto+Serif+JP&family=Cormorant+Garamond&family=Libre+Baskerville&family=Source+Serif+Pro&family=Crimson+Text&family=Inter&family=Crimson+Pro&family=Literata&family=Ubuntu+Mono&family=Inter&family=Roboto">
    <link rel="stylesheet" type="text/css" href="/css/style.css">

    
    

    <title>Jack Richter-Powell | Map Estimation</title>


    

</head><body class="container d-flex flex-column min-vh-100">

<div class="blog_nav_bar secondary_font ">
    
    
    <a class="navbar-brand" href="/">about</a>
    
    
    
    <a class="navbar-brand" href="/blog">« all posts</a>
    
    
</div>



<h3>
    <a class="title" href="/posts/diffautograd/">Map Estimation</a>
</h3>

<div class="reading_time secondary_font text-muted ">
    <span>
        Oct 24 2021 · 3 min read
    </span>

</div>





<h1 id="doing-tensor-calculus-with-autograd">Doing Tensor Calculus with Autograd</h1>
<h2 id="intro-to-tensors">Intro to Tensors</h2>
<h2 id="a-lightning-overview-of-differential-geometry">A lightning overview of differential geometry</h2>
<h3 id="smooth-manifolds">Smooth Manifolds</h3>
<p>A smooth manifold $M$ is a topological space endowed with a smooth structure, which is a collection of open sets on $M$ and diffeomorphisms $\varphi : M \to \R^n$ which biject that subset of $M$ to $\R^n$. The picture you should have in your head is that $M$ is locally Euclidean, similar to how the earth is locally flat. The dimension of $M$ is then said to be $n$, and the $\varphi$&rsquo;s are referred to as coordinates, since they allow us to describe points on $M$ using values in $\R^n$.</p>
<p>Since locally the manifold is Euclidean, we can attach a vector space to each point $p \in M$, which allows us to discuss vectors on the manifold. This vector space is usually denoted $T_p M$ and has the same dimension as $M$. You can &ldquo;bundle&rdquo; all these forms together to produce what&rsquo;s known as a tangent bundle $TM$, which you should think of as &ldquo;smoothly&rdquo; attaching a vector space ($T_p M$) to each point. Frustratingly, even though this assignment is smooth, one cannot compare vectors in spaces close &ndash; say $T_p M$ and $T_q M$ for $p,q$ &ldquo;close&rdquo; &ndash; to each other without running into headaches. We return to this issue below.</p>
<p>Now since we have a vector space at each point, one can construct a dual vector space to act on those. This is denoted $T_p^<em>(M)$, and as usual, contains all the linear functionals acting on elements of $T_p M$. Similar to the tangent bundle, you can bundle all these spaces together to produce what we call the Cotangent bundle $T^</em> M$.</p>
<h3 id="exterior-calculus">Exterior Calculus</h3>
<p>The central purpose of these constructions is that they allow you to extend vector calculus beyond the Euclidean space.</p>
<h3 id="riemannian-structures">Riemannian Structures</h3>
<p>A Riemmaninan manifold $(M,g)$ is a smooth manifold with a given bilinear function $g$ which at each point $p\in M$ gives
$$ g_p : T_p M \times T_p M  \to \R \qquad \text{ denoted } g_p(v,w) = \ang{v,w} \text{ for }v,w \in T_p M$$</p>
<h1 id="calculations-in-coordinates-using-autograd">Calculations in Coordinates using Autograd</h1>
<h2 id="autograd">Autograd</h2>
<p>Automatic differentiation &ndash; referred to colloquially as &ldquo;autograd&rdquo; &ndash; in the machine learning community, is a technique which allows the computation of arbitrary derivatives of functions defined in code. The idea is to exploit the fact that, regardless of what abstractions are at play, code is always reduced to simple arithmetic operations. One can then differentiate those instructions using the chain rule to compute derivatives.</p>
<p>The beauty is that, due to the existence of tensor libraries like Pytorch and more recently JAX, the end user does not need to concern themselves with the inner workings of automatic differentiation. These libraries expose high level API&rsquo;s which take in functions and return their gradients, or that allow the computation of efficient Jacobian vector products in linear time. The last part is of critical importance, since as we will see below, nearly all the important calculations in coordinates can be reduced to a combination of derivatives and JVP&rsquo;s.</p>
<h2 id="exterior-derivatives">Exterior Derivatives</h2>
<h2 id="pullbacks">Pullbacks</h2>
<p>An important construction in differential geometry is the pullback.</p>
<h2 id="lie-derivatives">Lie Derivatives</h2>
<p>There is an amazing identity for the exterior calculus which tells us that the Lie derivative satisfies the following
$$ L_X \omega = d(\iota_X \omega) + \iota_X (d\omega)$$</p>


<footer class="mt-auto d-flex justify-content-center text-muted small secondary_font">
    <span class="text-muted">Copyright (c) 2021, Jack Richter-Powell,
        <a class="text-muted" href="https://github.com/hadisinaee/avicenna" target="_blank"> created by Avicenna
            (MIT)</a>
    </span>
</footer><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx"
    crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/feather-icons/4.28.0/feather.min.js"></script>
<script>
    feather.replace()
</script></body>

</html>