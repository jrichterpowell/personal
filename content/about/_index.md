---
title: "about"
date: 2020-10-20T17:51:47+03:30
draft: false
headless: true

full_name: "Jessie Richter-Powell"
pronouns: "(they/she)"
profile_picture: "/static/me.jpg"
cv: "/static/resume.pdf"
# set to false if you don't want to show your blog
blog: true

socials:
    mail: "jrichterpowell@gmail.com"
    twitter: "jrichterpowell"
    github: "jrichterpowell"
    google_scholar: "citations?user=L78pVMMAAAAJ&hl=en"

interests:
    - Computational Physics / Chemistry 
    - PDE theory / Optimal Transport 
    - Differentiable Programming
    - Circuits and weird noises
academia:
    - course:
        degree: "PhD (did not complete)"
        institution: 'MIT'
        dep: 'Electrical Engineering / Computer Science'
        start_date: 2023
        end_date: 2024

    - course:
        degree: "B.A"
        institution: 'McGill University'
        major: 'Joint Hons. Math & CS'
        #minor: 'Statistics'
        start_date: '2017'
        end_date: '2021'
        other_info: 'Completed 9 courses at the graduate level, including core grad math curriculum'
---

Hi, I'm Jessie Richter-Powell. As of May 2025, I'm an intern at the [NVIDIA Toronto AI lab](https://research.nvidia.com/labs/toronto-ai/). Previously, I spent 2023-2024 at MIT in an EECS PhD program, until I got distracted by shiny things elsewhere. Before that, I was a visiting researcher under the supervision of [David Duvenaud](https://www.cs.toronto.edu/~duvenaud/) at the [Vector Institute](https://www.vectorinsitute.ai) from 2022-2023. Concurrently, I worked out of the MILA group headed by Yoshua Bengio for the summer of 2023, and still can be found frequently wandering that area.

Recently, my interests have pivoted towards work in the area of sound and machine learning. My recent preprint on using score distillation to program synthesizers and perform source separation, [Score Distillation Sampling for Audio](https://arxiv.org/abs/2505.04621), marks a first exploration in this space. 

Previously, my research was more concerned developing principled and structured learning algorithms with strong inductive biases derived from differential geometry, PDE theory and later physics. Put simply: stop learning principles you already know -- save the learning for the hard bits. 

An example of this can be seen in my NeurIPS 2022 paper, [Neural Conservation Laws](https://arxiv.org/abs/2210.01741) (along with [Ricky Chen](https://www.cs.toronto.edu/~rtqichen/) and [Yaron Lipman](https://www.wisdom.weizmann.ac.il/~ylipman/)). A brief synopsis: we exploited the classical identity d^2=0 to parameterize exact solutions of the continuity equation. This allows us to structurally enforce conservation of mass for applications like variable density fluid simulation and dynamical optimal transport.

Before that, I collaborated with [Rustum Choksi](https://www.math.mcgill.ca/rchoksi/) and [Carola Bibiane-Sch√∂nlieb](https://www.damtp.cam.ac.uk/user/cbs31/Home.html) on a data-driven extension of the [Maximum Entropy on the Mean Method for Image Deblurring / Denoising](https://arxiv.org/abs/2002.10434).

